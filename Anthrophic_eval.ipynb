{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "188fe278-7f10-4567-a9ac-1966a5739104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea398389-cca4-4762-85c3-41fdb8208491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_xml(text: str, tag: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the content of the specified XML tag from the given text. Used for parsing structured responses \n",
    "\n",
    "    Args:\n",
    "        text (str): The text containing the XML.\n",
    "        tag (str): The XML tag to extract content from.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the specified XML tag, or an empty string if the tag is not found.\n",
    "    \"\"\"\n",
    "    match = re.search(f'<{tag}>(.*?)</{tag}>', text, re.DOTALL)\n",
    "    return match.group(1) if match else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d103a59a-b0b2-4676-a210-e8e1e80405ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(prompt: str, system_prompt: str = \"\", model=\"llama3-70b-8192\") -> str:\n",
    "    \"\"\"\n",
    "    Calls the model with the given prompt and returns the response.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt to send to the model.\n",
    "        system_prompt (str, optional): The system prompt to send to the model. Defaults to \"\".\n",
    "        model (str, optional): The model to use for the call. Defaults to \"llama3-70b-8192\".\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the language model.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    llm = ChatGroq(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd6939d-08f7-4be9-9de6-f6c49f183fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, task: str, context: str = \"\") -> tuple[str, str]:\n",
    "    \"\"\"Generate and improve a solution based on feedback.\"\"\"\n",
    "    full_prompt = f\"{prompt}\\n{context}\\nTask: {task}\" if context else f\"{prompt}\\nTask: {task}\"\n",
    "    response = llm_call(full_prompt)\n",
    "    thoughts = extract_xml(response, \"thoughts\")\n",
    "    result = extract_xml(response, \"response\")\n",
    "    \n",
    "    print(\"\\n=== GENERATION START ===\")\n",
    "    print(f\"Thoughts:\\n{thoughts}\\n\")\n",
    "    print(f\"Generated:\\n{result}\")\n",
    "    print(\"=== GENERATION END ===\\n\")\n",
    "    \n",
    "    return thoughts, result\n",
    "\n",
    "def evaluate(prompt: str, content: str, task: str) -> tuple[str, str]:\n",
    "    \"\"\"Evaluate if a solution meets requirements.\"\"\"\n",
    "    full_prompt = f\"{prompt}\\nOriginal task: {task}\\nContent to evaluate: {content}\"\n",
    "    response = llm_call(full_prompt)\n",
    "    evaluation = extract_xml(response, \"evaluation\")\n",
    "    feedback = extract_xml(response, \"feedback\")\n",
    "    \n",
    "    print(\"=== EVALUATION START ===\")\n",
    "    print(f\"Status: {evaluation}\")\n",
    "    print(f\"Feedback: {feedback}\")\n",
    "    print(\"=== EVALUATION END ===\\n\")\n",
    "    \n",
    "    return evaluation, feedback\n",
    "\n",
    "def loop(task: str, evaluator_prompt: str, generator_prompt: str) -> tuple[str, list[dict]]:\n",
    "    \"\"\"Keep generating and evaluating until requirements are met.\"\"\"\n",
    "    memory = []\n",
    "    chain_of_thought = []\n",
    "    \n",
    "    thoughts, result = generate(generator_prompt, task)\n",
    "    memory.append(result)\n",
    "    chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})\n",
    "    \n",
    "    while True:\n",
    "        evaluation, feedback = evaluate(evaluator_prompt, result, task)\n",
    "        if evaluation == \"PASS\":\n",
    "            return result, chain_of_thought\n",
    "            \n",
    "        context = \"\\n\".join([\n",
    "            \"Previous attempts:\",\n",
    "            *[f\"- {m}\" for m in memory],\n",
    "            f\"\\nFeedback: {feedback}\"\n",
    "        ])\n",
    "        \n",
    "        thoughts, result = generate(generator_prompt, task, context)\n",
    "        memory.append(result)\n",
    "        chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a758fdc8-ffa7-468c-bf2b-66b8d6507d40",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 33\u001b[0m\n\u001b[0;32m      8\u001b[0m generator_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124mYour goal is to complete the task based on <user input>. If there are feedback \u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124mfrom your previous generations, you should reflect on them to improve your solution\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124m</response>\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     23\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124m<user input>\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124mImplement a Stack with:\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124m</user input>\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 33\u001b[0m result, chain_of_thought \u001b[38;5;241m=\u001b[39m \u001b[43mloop\u001b[49m(task, evaluator_prompt, generator_prompt)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loop' is not defined"
     ]
    }
   ],
   "source": [
    "evaluator_prompt = \"\"\"\n",
    "Evaluate this following code implementation for:\n",
    "1. code correctness\n",
    "2. time complexity\n",
    "3. style and best practices\n",
    "\"\"\"\n",
    "\n",
    "generator_prompt = \"\"\"\n",
    "Your goal is to complete the task based on <user input>. If there are feedback \n",
    "from your previous generations, you should reflect on them to improve your solution\n",
    "\n",
    "Output your answer concisely in the following format: \n",
    "\n",
    "<thoughts>\n",
    "[Your understanding of the task and feedback and how you plan to improve]\n",
    "</thoughts>\n",
    "\n",
    "<response>\n",
    "[Your code implementation here]\n",
    "</response>\n",
    "\"\"\"\n",
    "\n",
    "task = \"\"\"\n",
    "<user input>\n",
    "Implement a Stack with:\n",
    "1. push(x)\n",
    "2. pop()\n",
    "3. getMin()\n",
    "All operations should be O(1).\n",
    "</user input>\n",
    "\"\"\"\n",
    "\n",
    "result, chain_of_thought = loop(task, evaluator_prompt, generator_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc69dcb0-1205-4f76-a6ec-1668714762e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I understand that I need to implement a Stack data structure with three operations: push(x), pop(), and getMin(). The catch is that all these operations should be performed in O(1) time complexity. \\n\\nFrom the previous feedback, I learned that I should use two stacks to keep track of the minimum element at each push operation. This way, I can retrieve the minimum element in O(1) time.\\n\\nTo improve my solution, I will make sure to handle edge cases such as popping from an empty stack and pushing to a full stack.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_of_thought[0]['thoughts'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf6f8291-6e8b-46f6-b993-2985272ac76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```\\nclass MinStack:\\n\\n    def __init__(self):\\n        self.stack = []\\n        self.min_stack = []\\n\\n    def push(self, x: int) -> None:\\n        self.stack.append(x)\\n        if not self.min_stack or x <= self.min_stack[-1]:\\n            self.min_stack.append(x)\\n\\n    def pop(self) -> None:\\n        if self.stack:\\n            if self.stack[-1] == self.min_stack[-1]:\\n                self.min_stack.pop()\\n            self.stack.pop()\\n\\n    def getMin(self) -> int:\\n        if self.min_stack:\\n            return self.min_stack[-1]\\n        return None\\n```'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_of_thought[0]['result'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f07c9-631c-4b3b-810d-5f8dcb867f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
